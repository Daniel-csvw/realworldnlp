{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.7/site-packages (2.11.0)\n",
      "Requirement already satisfied: packaging in /Users/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.7/site-packages (from transformers) (19.0)\n",
      "Requirement already satisfied: tokenizers==0.7.0 in /Users/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.7/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.7/site-packages (from transformers) (0.1.83)\n",
      "Requirement already satisfied: requests in /Users/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.7/site-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: numpy in /Users/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.7/site-packages (from transformers) (1.16.1)\n",
      "Requirement already satisfied: sacremoses in /Users/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.7/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: filelock in /Users/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.7/site-packages (from transformers) (2018.1.10)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.7/site-packages (from transformers) (4.30.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.7/site-packages (from packaging->transformers) (2.3.1)\n",
      "Requirement already satisfied: six in /Users/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.7/site-packages (from packaging->transformers) (1.12.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Users/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.7/site-packages (from requests->transformers) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.7/site-packages (from requests->transformers) (2018.11.29)\n",
      "Requirement already satisfied: click in /Users/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /Users/mhagiwara/dev/realworldnlp/.pyenv/lib/python3.7/site-packages (from sacremoses->transformers) (0.15.1)\n",
      "\u001b[33mYou are using pip version 18.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k >0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p >0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        Source:\n",
    "            https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_token(output):\n",
    "    logits = output[..., -1, :].squeeze(0)\n",
    "    logits = top_k_top_p_filtering(logits, top_k=10)\n",
    "    log_probs = torch.softmax(logits, dim=-1)\n",
    "    token = torch.multinomial(log_probs, num_samples=1)[0]\n",
    "\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer-XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('transfo-xl-wt103')\n",
    "model = AutoModelWithLMHead.from_pretrained('transfo-xl-wt103')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = tokenizer.encode(\"On our way to the beach\")\n",
    "context = torch.tensor([generated])\n",
    "past = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    output, past = model(context, mems=past)\n",
    "    token = sample_token(output)\n",
    "\n",
    "    generated.append(token.item())\n",
    "    context = token.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On our way to the beach, she finds, she finds the men who are in the group to be \" in the group \". This has led to the perception that the \" group \" in the group is \" a group of people in the group with whom we share a deep friendship, and which is a common cause to the contrary. \" <eos> <eos> = = Background = = <eos> <eos> The origins of the concept of \" group \" were in early colonial years with the English Civil War. The term was coined by English abolitionist John\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")\n",
    "model = AutoModelWithLMHead.from_pretrained('gpt2-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = tokenizer.encode(\"Lynn Xiaoling Mo is a\")\n",
    "context = torch.tensor([generated])\n",
    "past = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    output, past = model(context, past=past)\n",
    "    token = sample_token(output)\n",
    "\n",
    "    generated.append(token.item())\n",
    "    context = token.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lynn Xiaoling Mo is a retired senior research scientist at Microsoft Research in Redmond, Washington. He received the 2013 Turing Award in recognition of his work on artificial brain-inspired systems. His latest work is focused on the development of an artificial neural network that learns from its environment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('xlm-clm-enfr-1024')\n",
    "model = AutoModelWithLMHead.from_pretrained('xlm-clm-enfr-1024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = [0] # start with just <s>\n",
    "context = torch.tensor([generated])\n",
    "lang = 0 # English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    langs = torch.zeros_like(context).fill_(lang)\n",
    "    output, = model(context, langs=langs)\n",
    "    token = sample_token(output)\n",
    "\n",
    "    generated.append(token.item())\n",
    "    context = torch.tensor([generated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>and its ability to make decisions on the basis of its own. \" </s>The government has taken no decisions on that matter, \" Mr Hockey said. </s>A lot of the information is very sensitive. </s>The new research and information on the Australian economy, which is what we're going to get from people, and the information that we are going to be looking at, we're going to be able to provide and we 'll take it forward. </s>I'm not trying to make sure we're not\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = [0] # start with just <s>\n",
    "context = torch.tensor([generated])\n",
    "lang = 1 # French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    langs = torch.zeros_like(context).fill_(lang)\n",
    "    output, = model(context, langs=langs)\n",
    "    token = sample_token(output)\n",
    "\n",
    "    generated.append(token.item())\n",
    "    context = torch.tensor([generated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s></s>En revanche, les prix des maisons individuelles n' ont guère augmenté ( - 0,1 % ). </s>En mars dernier, le taux de la taxe foncière, en légère augmentation à la hausse par rapport à février 2008. </s>\" Je n' ai jamais eu une augmentation \" précise \". </s>\" Je me suis toujours dit que ce n' était pas parce que c' était une blague. </s>En effet, j' étais un gars de la rue \" </s>Les jeunes sont des gens qui avaient beaucoup d' humour... \"\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
